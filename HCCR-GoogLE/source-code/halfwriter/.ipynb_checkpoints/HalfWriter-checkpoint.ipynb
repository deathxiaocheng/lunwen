{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # 导入TF库\n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential, metrics  # 导入TF子库\n",
    "import os, glob\n",
    "import random, csv\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建图片路径和标签，并写入csv文件\n",
    "def load_csv(root, filename, name2label):\n",
    "    # root:数据集根目录\n",
    "    # filename:csv文件名\n",
    "    # name2label:类别名编码表\n",
    "    if not os.path.exists(os.path.join(root, filename)):  # 如果不存在csv，则创建一个\n",
    "        images = []  # 初始化存放图片路径的字符串数组\n",
    "        for name in name2label.keys():  # 遍历所有子目录，获得所有图片的路径\n",
    "            # glob文件名匹配模式，不用遍历整个目录判断而获得文件夹下所有同类文件\n",
    "            # 只考虑后缀为png,jpg,jpeg的图片，比如：pokemon\\\\mewtwo\\\\00001.png\n",
    "            images += glob.glob(os.path.join(root, name, '*.png'))\n",
    "            images += glob.glob(os.path.join(root, name, '*.jpg'))\n",
    "            images += glob.glob(os.path.join(root, name, '*.jpeg'))\n",
    "        print(len(images), images)  # 打印出images的长度和所有图片路径名\n",
    "        random.shuffle(images)  # 随机打乱存放顺序\n",
    "        # 创建csv文件，并且写入图片路径和标签信息\n",
    "        with open(os.path.join(root, filename), mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for img in images:  # 遍历images中存放的每一个图片的路径，如pokemon\\\\mewtwo\\\\00001.png\n",
    "                name = img.split(os.sep)[-2]  # 用\\\\分隔，取倒数第二项作为类名\n",
    "                label = name2label[name]  # 找到类名键对应的值，作为标签\n",
    "                writer.writerow([img, label])  # 写入csv文件，以逗号隔开，如：pokemon\\\\mewtwo\\\\00001.png, 2\n",
    "            print('written into csv file:', filename)\n",
    "    # 读csv文件\n",
    "    images, labels = [], []  # 创建两个空数组，用来存放图片路径和标签\n",
    "    with open(os.path.join(root, filename)) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:  # 逐行遍历csv文件\n",
    "            img, label = row  # 每行信息包括图片路径和标签\n",
    "            label = int(label)  # 强制类型转换为整型\n",
    "            images.append(img)  # 插入到images数组的后面\n",
    "            labels.append(label)\n",
    "    assert len(images) == len(labels)  # 断言，判断images和labels的长度是否相同\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 首先遍历根目录下的所有子目录。对每个子目录，用类别名作为编码表的key，编码表的长度作为类别的标签，存进name2label字典对象\n",
    "def load_data(root):\n",
    "    # 创建数字编码表\n",
    "    name2label = {}  # 创建一个空字典{key:value}，用来存放类别名和对应的标签\n",
    "    for name in sorted(os.listdir(os.path.join(root))):  # 遍历根目录下的子目录，并排序\n",
    "        if not os.path.isdir(os.path.join(root, name)):  # 如果不是文件夹，则跳过\n",
    "            continue\n",
    "       \n",
    "        name2label[name] = len(name2label.keys())\n",
    "#         print(name) # 给每个类别编码一个数字\n",
    "#         print(len(name2label.keys()))\n",
    "    images, labels = load_csv(root, 'images.csv', name2label)  # 读取csv文件中已经写好的图片路径，和对应的标签\n",
    "    return images, labels, name2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_mean = tf.constant([0.485, 0.456, 0.406])\n",
    "img_std = tf.constant([0.229, 0.224, 0.225])\n",
    "def normalize(x, mean=img_mean, std=img_std):\n",
    "    x = (x - mean)/std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "def preprocess(image_path,label):\n",
    "    # x: 图片的路径，y：图片的数字编码\n",
    "    x = tf.io.read_file(image_path)  # 读入图片\n",
    "    x = tf.image.decode_jpeg(x, channels=3)  # 将原图解码为通道数为3的三维矩阵\n",
    "    x = tf.image.resize(x, [133, 133])\n",
    "    # 数据增强\n",
    "    # x = tf.image.random_flip_up_down(x) # 上下翻转\n",
    "    # x = tf.image.random_flip_left_right(x)  # 左右镜像\n",
    "    x = tf.image.random_crop(x, [113, 113, 3])  # 裁剪\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.  # 归一化\n",
    "    x = normalize(x)\n",
    "    y = tf.convert_to_tensor(label)  # 转换为张量\n",
    "#     y_onehot = tf.one_hot(y, depth=5) \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((None, 113, 113, 3), (None,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读入数据\n",
    "images, labels, table = load_data('/home/liuhengyu/dataset/data1/train/')\n",
    "# images_train = []\n",
    "# labels_train = []\n",
    "# for image in images:\n",
    "#     name = image.split(os.sep)[-2]\n",
    "#     if table[name] <= 300: \n",
    "#         images_train.append(image)\n",
    "#         labels_train.append(table[name])\n",
    "        \n",
    "num_classer = len(set(labels))\n",
    "print(num_classer)\n",
    "    \n",
    "    \n",
    "# print(images[10])\n",
    "# img = tf.io.read_file(images[10])\n",
    "# img = tf.image.decode_jpeg(img, channels=3)\n",
    "# img = tf.image.resize(img, [133, 133])\n",
    "# img = tf.image.random_crop(img, [113, 113, 3])  # 裁剪\n",
    "# img = tf.cast(img, dtype=tf.float32) / 255.  # 归一化\n",
    "# img = normalize(img)\n",
    "# # print(img.shape)\n",
    "# plt.figure(1)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((images, labels)) \n",
    "db = db.shuffle(1000).map(preprocess).batch(256).repeat(20)\n",
    "db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加入学习率衰减\n",
    "exponential_decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liuhengyu/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  7296      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  221440    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            multiple                  1327488   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  14156800  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  4024150   \n",
      "=================================================================\n",
      "Total params: 22,999,382\n",
      "Trainable params: 22,999,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2.网络搭建\n",
    "network = Sequential([\n",
    "    # 第一层\n",
    "    layers.Conv2D(96, kernel_size=5, strides=2, padding='same', activation='relu'),  # 55*55*48\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  \n",
    "    # 第二层\n",
    "    layers.Conv2D(256, kernel_size=3, strides=1, padding='same', activation='relu'),  # 27*27*128\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  # 13*13*128\n",
    "    # 第三层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu'),  # 13*13*192\n",
    "    # 第四层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu'),  # 13*13*192\n",
    "    # 第五层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu'),  # 13*13*128\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  # 6*6*128\n",
    "    layers.Flatten(),  # 6*6*128=4608\n",
    "    # 第六层\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    # 第七层\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    # 第八层（输出层）\n",
    "    layers.Dense(num_classer, activation='softmax')\n",
    "])\n",
    "network.compile(optimizer = tf.keras.optimizers.SGD(exponential_decay, momentum=0.9),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics = ['accuracy'])\n",
    "network.build(input_shape=(256, 113, 113, 3))  # 设置输入格式\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73340 steps\n",
      "   77/73340 [..............................] - ETA: 94:30:43 - loss: 8.2754 - accuracy: 3.5511e-04"
     ]
    }
   ],
   "source": [
    "history = network.fit(db, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-70b784adba54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.subplot(1, 2, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "print(acc[1])\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='accuracy')\n",
    "# plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.0]",
   "language": "python",
   "name": "conda-env-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
