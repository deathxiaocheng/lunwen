{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # 导入TF库\n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential, metrics  # 导入TF子库\n",
    "import os, glob\n",
    "import random, csv\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建图片路径和标签，并写入csv文件\n",
    "def load_csv(root, filename, name2label):\n",
    "    # root:数据集根目录\n",
    "    # filename:csv文件名\n",
    "    # name2label:类别名编码表\n",
    "    if not os.path.exists(os.path.join(root, filename)):  # 如果不存在csv，则创建一个\n",
    "        images = []  # 初始化存放图片路径的字符串数组\n",
    "        for name in name2label.keys():  # 遍历所有子目录，获得所有图片的路径\n",
    "            # glob文件名匹配模式，不用遍历整个目录判断而获得文件夹下所有同类文件\n",
    "            # 只考虑后缀为png,jpg,jpeg的图片，比如：pokemon\\\\mewtwo\\\\00001.png\n",
    "            images += glob.glob(os.path.join(root, name, '*.png'))\n",
    "            images += glob.glob(os.path.join(root, name, '*.jpg'))\n",
    "            images += glob.glob(os.path.join(root, name, '*.jpeg'))\n",
    "        print(len(images), images)  # 打印出images的长度和所有图片路径名\n",
    "        random.shuffle(images)  # 随机打乱存放顺序\n",
    "        # 创建csv文件，并且写入图片路径和标签信息\n",
    "        with open(os.path.join(root, filename), mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for img in images:  # 遍历images中存放的每一个图片的路径，如pokemon\\\\mewtwo\\\\00001.png\n",
    "                name = img.split(os.sep)[-2]  # 用\\\\分隔，取倒数第二项作为类名\n",
    "                label = name2label[name]  # 找到类名键对应的值，作为标签\n",
    "                writer.writerow([img, label])  # 写入csv文件，以逗号隔开，如：pokemon\\\\mewtwo\\\\00001.png, 2\n",
    "            print('written into csv file:', filename)\n",
    "    # 读csv文件\n",
    "    images, labels = [], []  # 创建两个空数组，用来存放图片路径和标签\n",
    "    with open(os.path.join(root, filename)) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:  # 逐行遍历csv文件\n",
    "            img, label = row  # 每行信息包括图片路径和标签\n",
    "            label = float(label)  # 强制类型转换为整型\n",
    "            images.append(img)  # 插入到images数组的后面\n",
    "            labels.append(label)\n",
    "    assert len(images) == len(labels)  # 断言，判断images和labels的长度是否相同\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 首先遍历根目录下的所有子目录。对每个子目录，用类别名作为编码表的key，编码表的长度作为类别的标签，存进name2label字典对象\n",
    "def load_data(root):\n",
    "    # 创建数字编码表\n",
    "    name2label = {}  # 创建一个空字典{key:value}，用来存放类别名和对应的标签\n",
    "    for name in sorted(os.listdir(os.path.join(root))):  # 遍历根目录下的子目录，并排序\n",
    "        if not os.path.isdir(os.path.join(root, name)):  # 如果不是文件夹，则跳过\n",
    "            continue\n",
    "       \n",
    "        name2label[name] = len(name2label.keys())\n",
    "#         print(name) # 给每个类别编码一个数字\n",
    "#         print(len(name2label.keys()))\n",
    "    images, labels = load_csv(root, 'images.csv', name2label)  # 读取csv文件中已经写好的图片路径，和对应的标签\n",
    "    return images, labels, name2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_mean = tf.constant([0.485, 0.456, 0.406])\n",
    "img_std = tf.constant([0.229, 0.224, 0.225])\n",
    "def normalize(x, mean=img_mean, std=img_std):\n",
    "    x = (x - mean)/std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "def preprocess(image_path,label):\n",
    "    # x: 图片的路径，y：图片的数字编码\n",
    "    x = tf.io.read_file(image_path)  # 读入图片\n",
    "    x = tf.image.decode_jpeg(x, channels=3)  # 将原图解码为通道数为3的三维矩阵\n",
    "    x = tf.image.resize(x, [133, 133])\n",
    "    # 数据增强\n",
    "    # x = tf.image.random_flip_up_down(x) # 上下翻转\n",
    "    # x = tf.image.random_flip_left_right(x)  # 左右镜像\n",
    "    x = tf.image.random_crop(x, [113, 113, 3])  # 裁剪\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.  # 归一化\n",
    "    x = normalize(x)\n",
    "    y = tf.convert_to_tensor(label)  # 转换为张量\n",
    "#     y_onehot = tf.one_hot(y, depth=5) \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((None, 113, 113, 3), (None,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读入数据\n",
    "images, labels, table = load_data('/home/liuhengyu/dataset/data1/train/')\n",
    "# images_train = []\n",
    "# labels_train = []\n",
    "# for image in images:\n",
    "#     name = image.split(os.sep)[-2]\n",
    "#     if table[name] <= 300: \n",
    "#         images_train.append(image)\n",
    "#         labels_train.append(table[name])\n",
    "        \n",
    "num_classer = len(set(labels))\n",
    "print(num_classer)\n",
    "    \n",
    "    \n",
    "# print(images[10])\n",
    "# img = tf.io.read_file(images[10])\n",
    "# img = tf.image.decode_jpeg(img, channels=3)\n",
    "# img = tf.image.resize(img, [133, 133])\n",
    "# img = tf.image.random_crop(img, [113, 113, 3])  # 裁剪\n",
    "# img = tf.cast(img, dtype=tf.float32) / 255.  # 归一化\n",
    "# img = normalize(img)\n",
    "# # print(img.shape)\n",
    "# plt.figure(1)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((images, labels)) \n",
    "db = db.shuffle(20000).map(preprocess).batch(256).repeat(20)\n",
    "db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加入学习率衰减\n",
    "exponential_decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=0.1, decay_steps=10000, decay_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           multiple                  7296      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           multiple                  221440    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           multiple                  885120    \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           multiple                  1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           multiple                  1327488   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  14156800  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  1049600   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  4024150   \n",
      "=================================================================\n",
      "Total params: 22,999,382\n",
      "Trainable params: 22,999,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2.网络搭建\n",
    "network = Sequential([\n",
    "    # 第一层\n",
    "    layers.Conv2D(96, kernel_size=5, strides=2, padding='same', activation='relu'),  # 55*55*48\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  \n",
    "    # 第二层\n",
    "    layers.Conv2D(256, kernel_size=3, strides=1, padding='same', activation='relu'),  # 27*27*128\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  # 13*13*128\n",
    "    # 第三层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu'),  # 13*13*192\n",
    "    # 第四层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu'),  # 13*13*192\n",
    "    # 第五层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu'),  # 13*13*128\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  # 6*6*128\n",
    "    layers.Flatten(),  # 6*6*128=4608\n",
    "    # 第六层\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    # 第七层\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    # 第八层（输出层）\n",
    "    layers.Dense(num_classer, activation='softmax')\n",
    "])\n",
    "network.compile(optimizer = tf.keras.optimizers.SGD(exponential_decay, momentum=0.9),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics = ['accuracy'])\n",
    "network.build(input_shape=(256, 113, 113, 3))  # 设置输入格式\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73340 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-02706a7bdb90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3575\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3576\u001b[0m         session != self._session):\n\u001b[0;32m-> 3577\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3579\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
      "\u001b[0;32m~/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   3512\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m     \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3514\u001b[0;31m     \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3515\u001b[0m     \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3516\u001b[0m     \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \"\"\"\n\u001b[1;32m   1504\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2.0/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1458\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1460\u001b[0;31m             session._session, options_ptr)\n\u001b[0m\u001b[1;32m   1461\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = network.fit(db, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-70b784adba54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.subplot(1, 2, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "print(acc[1])\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='accuracy')\n",
    "# plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.0]",
   "language": "python",
   "name": "conda-env-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
